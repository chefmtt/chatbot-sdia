{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Our first approach is to generate, by hand, a simple collections of sentences organised by intents and associated answers.\n",
    "We will then process that data to make it suitable for NLP applications, encode it using \"bag of word\" and train a neural network to predict user intent from an utterance.\n",
    "\n",
    "Then, we wil used pre-trained word embeddings\n",
    "\n",
    "Then, we will generate training data using OpenAI GPT-3 API and train it, using both bag of words and word embeddings\n",
    "We can also try TF-IDF to compare it with the NN."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A chatbot needs to understand intents in users' utterances. For this purpose, we train a classifier."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "I use the IMDB review dataset for basic testing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import / Generate data\n",
    "\n",
    "In this section, we import our dataset, made of hand-crafted sentences and the corresponding intent."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import tree, svm, naive_bayes\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "data_file = open('intents.json').read()\n",
    "intents = json.loads(data_file)\n",
    "\n",
    "\n",
    "data = []\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        data.append([pattern, intent['tag']])\n",
    "\n",
    "df = pd.DataFrame(data, columns=['text','intent'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                review sentiment\n",
      "0    One of the other reviewers has mentioned that ...  positive\n",
      "1    A wonderful little production. <br /><br />The...  positive\n",
      "2    I thought this was a wonderful way to spend ti...  positive\n",
      "3    Basically there's a family where a little boy ...  negative\n",
      "4    Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "..                                                 ...       ...\n",
      "995  Nothing is sacred. Just ask Ernie Fosselius. T...  positive\n",
      "996  I hated it. I hate self-aware pretentious inan...  negative\n",
      "997  I usually try to be professional and construct...  negative\n",
      "998  If you like me is going to see this in a film ...  negative\n",
      "999  This is like a zoology textbook, given that it...  negative\n",
      "\n",
      "[1000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df_imdb = pd.read_csv(\"IMDB Dataset.csv\", nrows=1000)\n",
    "print(df_imdb)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The intents that we want the chatbot to recognize are :"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[\"intent\"].unique()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data pre-processing\n",
    "\n",
    "In this section, we define functions to preprocess our text (parse it using a SpaCy pipeline) and to process it (extract tokens, lemmas or embeddings depending on the application).\n",
    "We save the preprocessed data to disk to avoid repeating this computationally expensive task."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# fine-tune preprocessing for spaCy word embeddings using this method : https://www.kaggle.com/code/christofhenkel/how-to-preprocessing-when-using-embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def lemmatize_text(text, preprocessed=True):\n",
    "    return process_text(text, \"lemmatize\", preprocessed)\n",
    "\n",
    "def tokenize_text(text, preprocessed=True):\n",
    "    return process_text(text, \"tokenize\", preprocessed)\n",
    "\n",
    "def process_text(text, mode: str, preprocessed=True):\n",
    "    if not preprocessed:\n",
    "        text = nlp(text)\n",
    "    if mode == \"tokenize\":\n",
    "        processed_text = [token.text for token in text] # token and embed must have the same processing + SpaCy provides embeddings for punctuation\n",
    "    elif mode == \"embed\":\n",
    "        processed_text = [token.vector for token in text] # token and embed must have the same processing\n",
    "    elif mode == \"lemmatize\":\n",
    "        processed_text = [token.lemma_ for token in text\n",
    "                               if not token.is_punct and not token.is_space and not token.like_url and not token.like_email]\n",
    "    else:\n",
    "        raise ValueError(\"Mode not supported\")\n",
    "    return processed_text\n",
    "\n",
    "def save_preprocessed(raw_text, save_path):\n",
    "    doc_bin = DocBin(attrs=[\"LEMMA\", \"ENT_IOB\", \"ENT_TYPE\"], store_user_data=True)\n",
    "    for doc in nlp.pipe(raw_text):\n",
    "        doc_bin.add(doc)\n",
    "    # save DocBin to a file on disc\n",
    "    doc_bin.to_disk(save_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "file_name_spacy = 'preprocessed_imdb.spacy'\n",
    "#save_preprocessed(raw_text=df_imdb[\"review\"], save_path=file_name_spacy)\n",
    "\n",
    "# Load DocBin at later time or on different system from disc or bytes object\n",
    "doc_bin = DocBin().from_disk(file_name_spacy)\n",
    "df_imdb[\"doc\"] = list(doc_bin.get_docs(nlp.vocab))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                review sentiment  \\\n",
      "0    One of the other reviewers has mentioned that ...  positive   \n",
      "1    A wonderful little production. <br /><br />The...  positive   \n",
      "2    I thought this was a wonderful way to spend ti...  positive   \n",
      "3    Basically there's a family where a little boy ...  negative   \n",
      "4    Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
      "..                                                 ...       ...   \n",
      "995  Nothing is sacred. Just ask Ernie Fosselius. T...  positive   \n",
      "996  I hated it. I hate self-aware pretentious inan...  negative   \n",
      "997  I usually try to be professional and construct...  negative   \n",
      "998  If you like me is going to see this in a film ...  negative   \n",
      "999  This is like a zoology textbook, given that it...  negative   \n",
      "\n",
      "                                                   doc  \n",
      "0    (One, of, the, other, reviewers, has, mentione...  \n",
      "1    (A, wonderful, little, production, ., <, br, /...  \n",
      "2    (I, thought, this, was, a, wonderful, way, to,...  \n",
      "3    (Basically, there, 's, a, family, where, a, li...  \n",
      "4    (Petter, Mattei, 's, \", Love, in, the, Time, o...  \n",
      "..                                                 ...  \n",
      "995  (Nothing, is, sacred, ., Just, ask, Ernie, Fos...  \n",
      "996  (I, hated, it, ., I, hate, self, -, aware, pre...  \n",
      "997  (I, usually, try, to, be, professional, and, c...  \n",
      "998  (If, you, like, me, is, going, to, see, this, ...  \n",
      "999  (This, is, like, a, zoology, textbook, ,, give...  \n",
      "\n",
      "[1000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_imdb)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data preparation\n",
    "\n",
    "In this section, we create different training datasets, processing them using SpaCy and our helper functions :\n",
    "\n",
    "- `X_train` is a pandas Series made of all preprocessed sentence\n",
    "- `X_train_embedded` pandas Series, each sentence is a list of embeddings\n",
    "- `X_train_embedded_avg` panda Series, each sentence is the average of each of its words' embedding (using the sum would give embeddings of different magnitude depending of the sentence's length)\n",
    "- `X_train_embedded_avg_tfidf` The previous average is weighted using TF-IDF coefficient (trained on ngrams of 1 token)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "train, test = train_test_split(df_imdb, test_size=0.3)\n",
    "\n",
    "X_train = train[\"doc\"].reset_index(drop=True)\n",
    "y_train = train[\"sentiment\"].reset_index(drop=True)\n",
    "\n",
    "X_test = test[\"doc\"].reset_index(drop=True)\n",
    "y_test = test[\"sentiment\"].reset_index(drop=True)\n",
    "\n",
    "X_train_embedded = train[\"doc\"].apply(process_text, args=(\"embed\", True,))\n",
    "X_train_embedded_avg = X_train_embedded.apply(np.mean, axis=0).apply(pd.Series)\n",
    "\n",
    "X_test_embedded = test[\"doc\"].apply(process_text, args=(\"embed\", True,))\n",
    "X_test_embedded_avg = X_test_embedded.apply(np.mean, axis=0).apply(pd.Series)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [10]\u001B[0m, in \u001B[0;36m<cell line: 8>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     13\u001B[0m         \u001B[38;5;28;01mexcept\u001B[39;00m(\u001B[38;5;167;01mKeyError\u001B[39;00m):\n\u001B[1;32m     14\u001B[0m             \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m---> 15\u001B[0m         sum_embeddings \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (\u001B[43mX_train_tfidf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtoarray\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)[idxRow][tfidf_idx] \u001B[38;5;241m*\u001B[39m X_train_embedded\u001B[38;5;241m.\u001B[39miloc[idxRow][idxWord]\n\u001B[1;32m     16\u001B[0m     weighted_averages\u001B[38;5;241m.\u001B[39mappend(sum_embeddings\u001B[38;5;241m/\u001B[39m\u001B[38;5;28mlen\u001B[39m(sentence))\n\u001B[1;32m     18\u001B[0m X_train_embedded_avg_tfidf \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mSeries(weighted_averages)\u001B[38;5;241m.\u001B[39mapply(pd\u001B[38;5;241m.\u001B[39mSeries)\n",
      "File \u001B[0;32m~/miniconda3/envs/chatbot-sdia/lib/python3.9/site-packages/scipy/sparse/_compressed.py:1062\u001B[0m, in \u001B[0;36m_cs_matrix.toarray\u001B[0;34m(self, order, out)\u001B[0m\n\u001B[1;32m   1060\u001B[0m     y \u001B[38;5;241m=\u001B[39m out\u001B[38;5;241m.\u001B[39mT\n\u001B[1;32m   1061\u001B[0m M, N \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39m_swap(x\u001B[38;5;241m.\u001B[39mshape)\n\u001B[0;32m-> 1062\u001B[0m \u001B[43mcsr_todense\u001B[49m\u001B[43m(\u001B[49m\u001B[43mM\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mN\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindptr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindices\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1063\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# The following code block construct a sentence representation as the average of all embeddings of the words in it, weighted by their tfidf score\n",
    "# This is not practical in our chatbot : using word embeddings is one way of mitigating the small dataset size, as words close in meaning should have similar embeddings\n",
    "# Weighing by tf-idf score would \"delete\" unknown world from the vocabulary, which we do not want\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 1), lowercase=False, tokenizer=tokenize_text, max_features=10000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train) # Maybe not ? Bias, vocab for test in vect // But that would be dumb to not use the vocab for the final one // BEST : Only do vocab on X_train, but if tfidf selected train final on FULL dataset\n",
    "weighted_averages = []\n",
    "for (idxRow, sentence) in X_train.items():\n",
    "    sum_embeddings = 0\n",
    "    for idxWord, word in enumerate(sentence):\n",
    "        try:\n",
    "            tfidf_idx = vectorizer.vocabulary_[word]\n",
    "        except(KeyError):\n",
    "            continue\n",
    "        sum_embeddings += (X_train_tfidf.toarray())[idxRow][tfidf_idx] * X_train_embedded.iloc[idxRow][idxWord]\n",
    "    weighted_averages.append(sum_embeddings/len(sentence))\n",
    "\n",
    "X_train_embedded_avg_tfidf = pd.Series(weighted_averages).apply(pd.Series)\n",
    "\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "weighted_averages = []\n",
    "for (idxRow, sentence) in X_test.items():\n",
    "    sum_embeddings = 0\n",
    "    for idxWord, word in enumerate(sentence):\n",
    "        try:\n",
    "            tfidf_idx = vectorizer.vocabulary_[word]\n",
    "        except(KeyError):\n",
    "            continue\n",
    "        sum_embeddings += (X_test_tfidf.toarray())[idxRow][tfidf_idx] * X_test_embedded.iloc[idxRow][idxWord]\n",
    "    weighted_averages.append(sum_embeddings/len(sentence))\n",
    "\n",
    "X_test_embedded_avg_tfidf = pd.Series(weighted_averages).apply(pd.Series)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TODO : Explain alternatives (sense2vec, Doc2vec)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classic ML\n",
    "\n",
    "Our first approach to create our classifier is to use traditional ML algorithms.\n",
    "\n",
    "We will use several algorithms and 3 different approach to represent our training data :\n",
    "\n",
    "- A classic TF-IDF representation (with or without IDF, which is equivalent to a bag-of-words approach)\n",
    "- A \"sentence2vec\" (or s2v) approach, where a sentence is the average of its words' embedding.\n",
    "- A TF-IDF weighted average of word embeddings, s2v_tfidf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Models preparation\n",
    "\n",
    "We use the sklearn implementation of GridSearchCV, which optimises the parameters of an estimator (here, our classifiers) by cross-validated grid-search over a parameter grid.\n",
    "We select different algorithms, define a pipeline and a set of parameters for each of those.\n",
    "The use of the pipeline allows us to select the best parameters for the TF-IDF vectorization.\n",
    "\n",
    "The size of our dataset does not contrains us in the choice of the algorithm, as training time is not a concern (No need to swap SVC for LinearSVC or SGDClassifier, for example)\n",
    "\n",
    "GridSearchCV uses K-fold as the cross-validation method. Here, we used 5-fold stratified K-fold.\n",
    "\n",
    "List parameters for clf and vectorizer ?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below are defined the models and their corresponding hyperparameters to tune for the TF-IDF approach"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(lowercase=False, tokenizer=lemmatize_text, max_features=3000)\n",
    "\n",
    "gs_dict_tfidf = defaultdict(dict)\n",
    "\n",
    "dectree = tree.DecisionTreeClassifier() # CART\n",
    "svm_clf = svm.SVC()\n",
    "multi_nb = naive_bayes.MultinomialNB() # Not suitable for negative values (thus not suitable for word embeddings)\n",
    "log_reg = LogisticRegression()\n",
    "random_forest = RandomForestClassifier()\n",
    "skboost = GradientBoostingClassifier()\n",
    "\n",
    "gs_dict_tfidf['dectree']['pipeline'] = Pipeline([\n",
    "    ('vect', vect),\n",
    "    ('dectree', dectree)])\n",
    "gs_dict_tfidf['svm_clf']['pipeline'] = Pipeline([\n",
    "    ('vect', vect),\n",
    "    ('svm_clf', svm_clf)])\n",
    "gs_dict_tfidf['multi_nb']['pipeline'] = Pipeline([\n",
    "    ('vect', vect),\n",
    "    ('multi_nb', multi_nb)])\n",
    "gs_dict_tfidf['log_reg']['pipeline'] = Pipeline([\n",
    "    ('vect', vect),\n",
    "    ('log_reg', log_reg)])\n",
    "gs_dict_tfidf['random_forest']['pipeline'] = Pipeline([\n",
    "    ('vect', vect),\n",
    "    ('random_forest', random_forest)])\n",
    "gs_dict_tfidf['skboost']['pipeline'] = Pipeline([\n",
    "    ('vect', vect),\n",
    "    ('skboost', skboost)])\n",
    "\n",
    "gs_dict_tfidf['dectree']['params'] = {\n",
    "    \"dectree__max_depth\": [4, 40],\n",
    "    \"vect__ngram_range\": ((1, 1), (1, 2), (1,3), (1,4)),\n",
    "    \"vect__use_idf\": (True, False),\n",
    "    \"vect__binary\": (True, False),\n",
    "}\n",
    "gs_dict_tfidf['svm_clf']['params'] = {\n",
    "    \"svm_clf__kernel\": [\"linear\", \"rbf\"],\n",
    "    \"vect__ngram_range\": ((1, 1), (1, 2), (1,3), (1,4)),\n",
    "    \"vect__use_idf\": (True, False),\n",
    "    \"vect__binary\": (True, False),\n",
    "}\n",
    "gs_dict_tfidf['multi_nb']['params'] = {\n",
    "    \"multi_nb__alpha\": [0.00001, 0.0001, 0.001, 0.1, 1, 10, 100,1000],\n",
    "    \"vect__ngram_range\": ((1, 1), (1, 2), (1,3), (1,4)),\n",
    "    \"vect__use_idf\": (True, False),\n",
    "    \"vect__binary\": (True, False),\n",
    "}\n",
    "gs_dict_tfidf['log_reg']['params'] = {\n",
    "    \"vect__ngram_range\": ((1, 1), (1, 2), (1,3), (1,4)),\n",
    "    \"vect__use_idf\": (True, False),\n",
    "    \"vect__binary\": (True, False),\n",
    "}\n",
    "gs_dict_tfidf['random_forest']['params'] = {\n",
    "    \"vect__ngram_range\": ((1, 1), (1, 2), (1,3), (1,4)),\n",
    "    \"vect__use_idf\": (True, False),\n",
    "    \"vect__binary\": (True, False),\n",
    "}\n",
    "gs_dict_tfidf['skboost']['params'] = {\n",
    "    \"vect__ngram_range\": ((1, 1), (1, 2), (1,3), (1,4)),\n",
    "    \"vect__use_idf\": (True, False),\n",
    "    \"vect__binary\": (True, False),\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below are defined the models to be used with the two embeddings approaches"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "gs_dict_embeddings = defaultdict(dict)\n",
    "# classifiers to use\n",
    "dectree = tree.DecisionTreeClassifier()\n",
    "svm_clf = svm.SVC()\n",
    "\n",
    "gs_dict_embeddings['dectree']['pipeline'] = Pipeline([\n",
    "    ('dectree', dectree)])\n",
    "gs_dict_embeddings['svm_clf']['pipeline'] = Pipeline([\n",
    "    ('svm_clf', svm_clf)])\n",
    "gs_dict_embeddings['log_reg']['pipeline'] = Pipeline([\n",
    "    ('log_reg', log_reg)])\n",
    "gs_dict_embeddings['random_forest']['pipeline'] = Pipeline([\n",
    "    ('random_forest', random_forest)])\n",
    "gs_dict_embeddings['skboost']['pipeline'] = Pipeline([\n",
    "    ('skboost', skboost)])\n",
    "\n",
    "gs_dict_embeddings['dectree']['params'] = {\n",
    "    \"dectree__max_depth\": [4, 10],\n",
    "}\n",
    "gs_dict_embeddings['svm_clf']['params'] = {\n",
    "    \"svm_clf__kernel\": [\"linear\", \"rbf\"],\n",
    "}\n",
    "gs_dict_embeddings['log_reg']['params'] = {\n",
    "\n",
    "}\n",
    "gs_dict_embeddings['random_forest']['params'] = {\n",
    "\n",
    "}\n",
    "gs_dict_embeddings['skboost']['params'] = {\n",
    "\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Selection"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def perform_grid_search(X_train, y_train, pipeline, parameters, scoring):\n",
    "    gs_clf = GridSearchCV(pipeline, parameters, n_jobs=1, verbose=1, cv=3, scoring=scoring) # Issue when n_jobs = -1 OR > 1\n",
    "    # I believe this may be because we use a custom tokenizer in TfidfVectorizer(), can't find how to solve it\n",
    "    print(\"\\n------------------------------------------------------------------------\\n\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "\n",
    "    t0 = time()\n",
    "\n",
    "    gs_clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"\\nDone in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % gs_clf.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(f\"\\t'{param_name}': '{best_parameters[param_name]}'\")\n",
    "    return gs_clf\n",
    "\n",
    "def best_estimator_per_clf(X_train, y_train, gs_dict: defaultdict, scoring):\n",
    "    for clf in dict(gs_dict):\n",
    "        gs_dict[clf]['gs'] = perform_grid_search(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            gs_dict[clf]['pipeline'],\n",
    "            gs_dict[clf]['params'],\n",
    "            scoring\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "pipeline: ['vect', 'dectree']\n",
      "parameters:\n",
      "{'dectree__max_depth': [4, 40],\n",
      " 'vect__binary': (True, False),\n",
      " 'vect__ngram_range': ((1, 1), (1, 2), (1, 3), (1, 4)),\n",
      " 'vect__use_idf': (True, False)}\n",
      "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n",
      "\n",
      "Done in 48.211s\n",
      "\n",
      "Best score: 0.691\n",
      "Best parameters set:\n",
      "\t'dectree__max_depth': '40'\n",
      "\t'vect__binary': 'False'\n",
      "\t'vect__ngram_range': '(1, 1)'\n",
      "\t'vect__use_idf': 'True'\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "pipeline: ['vect', 'svm_clf']\n",
      "parameters:\n",
      "{'svm_clf__kernel': ['linear', 'rbf'],\n",
      " 'vect__binary': (True, False),\n",
      " 'vect__ngram_range': ((1, 1), (1, 2), (1, 3), (1, 4)),\n",
      " 'vect__use_idf': (True, False)}\n",
      "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n",
      "\n",
      "Done in 75.772s\n",
      "\n",
      "Best score: 0.829\n",
      "Best parameters set:\n",
      "\t'svm_clf__kernel': 'linear'\n",
      "\t'vect__binary': 'False'\n",
      "\t'vect__ngram_range': '(1, 4)'\n",
      "\t'vect__use_idf': 'True'\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "pipeline: ['vect', 'multi_nb']\n",
      "parameters:\n",
      "{'multi_nb__alpha': [1e-05, 0.0001, 0.001, 0.1, 1, 10, 100, 1000],\n",
      " 'vect__binary': (True, False),\n",
      " 'vect__ngram_range': ((1, 1), (1, 2), (1, 3), (1, 4)),\n",
      " 'vect__use_idf': (True, False)}\n",
      "Fitting 3 folds for each of 128 candidates, totalling 384 fits\n",
      "\n",
      "Done in 177.768s\n",
      "\n",
      "Best score: 0.824\n",
      "Best parameters set:\n",
      "\t'multi_nb__alpha': '1'\n",
      "\t'vect__binary': 'True'\n",
      "\t'vect__ngram_range': '(1, 1)'\n",
      "\t'vect__use_idf': 'False'\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "pipeline: ['vect', 'log_reg']\n",
      "parameters:\n",
      "{'vect__binary': (True, False),\n",
      " 'vect__ngram_range': ((1, 1), (1, 2), (1, 3), (1, 4)),\n",
      " 'vect__use_idf': (True, False)}\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "\n",
      "Done in 22.422s\n",
      "\n",
      "Best score: 0.833\n",
      "Best parameters set:\n",
      "\t'vect__binary': 'True'\n",
      "\t'vect__ngram_range': '(1, 1)'\n",
      "\t'vect__use_idf': 'True'\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "pipeline: ['vect', 'random_forest']\n",
      "parameters:\n",
      "{'vect__binary': (True, False),\n",
      " 'vect__ngram_range': ((1, 1), (1, 2), (1, 3), (1, 4)),\n",
      " 'vect__use_idf': (True, False)}\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "\n",
      "Done in 33.531s\n",
      "\n",
      "Best score: 0.780\n",
      "Best parameters set:\n",
      "\t'vect__binary': 'False'\n",
      "\t'vect__ngram_range': '(1, 2)'\n",
      "\t'vect__use_idf': 'False'\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "pipeline: ['vect', 'skboost']\n",
      "parameters:\n",
      "{'vect__binary': (True, False),\n",
      " 'vect__ngram_range': ((1, 1), (1, 2), (1, 3), (1, 4)),\n",
      " 'vect__use_idf': (True, False)}\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "\n",
      "Done in 85.777s\n",
      "\n",
      "Best score: 0.751\n",
      "Best parameters set:\n",
      "\t'vect__binary': 'False'\n",
      "\t'vect__ngram_range': '(1, 1)'\n",
      "\t'vect__use_idf': 'True'\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "pipeline: ['dectree']\n",
      "parameters:\n",
      "{'dectree__max_depth': [4, 10]}\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "\n",
      "Done in 0.394s\n",
      "\n",
      "Best score: 0.667\n",
      "Best parameters set:\n",
      "\t'dectree__max_depth': '4'\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "pipeline: ['svm_clf']\n",
      "parameters:\n",
      "{'svm_clf__kernel': ['linear', 'rbf']}\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "\n",
      "Done in 0.229s\n",
      "\n",
      "Best score: 0.829\n",
      "Best parameters set:\n",
      "\t'svm_clf__kernel': 'linear'\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "pipeline: ['log_reg']\n",
      "parameters:\n",
      "{}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "\n",
      "Done in 0.065s\n",
      "\n",
      "Best score: 0.816\n",
      "Best parameters set:\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "pipeline: ['vect', 'random_forest']\n",
      "parameters:\n",
      "{}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 3 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n3 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/home/matthieu/miniconda3/envs/chatbot-sdia/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/home/matthieu/miniconda3/envs/chatbot-sdia/lib/python3.9/site-packages/sklearn/pipeline.py\", line 378, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"/home/matthieu/miniconda3/envs/chatbot-sdia/lib/python3.9/site-packages/sklearn/pipeline.py\", line 336, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"/home/matthieu/miniconda3/envs/chatbot-sdia/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n    return self.func(*args, **kwargs)\n  File \"/home/matthieu/miniconda3/envs/chatbot-sdia/lib/python3.9/site-packages/sklearn/pipeline.py\", line 870, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"/home/matthieu/miniconda3/envs/chatbot-sdia/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 2079, in fit_transform\n    X = super().fit_transform(raw_documents)\n  File \"/home/matthieu/miniconda3/envs/chatbot-sdia/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1338, in fit_transform\n    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n  File \"/home/matthieu/miniconda3/envs/chatbot-sdia/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1209, in _count_vocab\n    for feature in analyze(doc):\n  File \"/home/matthieu/miniconda3/envs/chatbot-sdia/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 113, in _analyze\n    doc = tokenizer(doc)\n  File \"/tmp/ipykernel_17605/2133088996.py\", line 4, in lemmatize_text\n    return process_text(text, \"lemmatize\", preprocessed)\n  File \"/tmp/ipykernel_17605/2133088996.py\", line 17, in process_text\n    processed_text = [token.lemma_ for token in text\nTypeError: 'int' object is not iterable\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [17]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m best_estimator_per_clf(X_train, y_train, gs_dict_tfidf, scoring\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 2\u001B[0m \u001B[43mbest_estimator_per_clf\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train_embedded_avg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgs_dict_embeddings\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscoring\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43maccuracy\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m best_estimator_per_clf(X_train_embedded_avg_tfidf, y_train, gs_dict_embeddings, scoring\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Input \u001B[0;32mIn [16]\u001B[0m, in \u001B[0;36mbest_estimator_per_clf\u001B[0;34m(X_train, y_train, gs_dict, scoring)\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbest_estimator_per_clf\u001B[39m(X_train, y_train, gs_dict: defaultdict, scoring):\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m clf \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mdict\u001B[39m(gs_dict):\n\u001B[0;32m---> 27\u001B[0m         gs_dict[clf][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgs\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mperform_grid_search\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     28\u001B[0m \u001B[43m            \u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     29\u001B[0m \u001B[43m            \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     30\u001B[0m \u001B[43m            \u001B[49m\u001B[43mgs_dict\u001B[49m\u001B[43m[\u001B[49m\u001B[43mclf\u001B[49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpipeline\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     31\u001B[0m \u001B[43m            \u001B[49m\u001B[43mgs_dict\u001B[49m\u001B[43m[\u001B[49m\u001B[43mclf\u001B[49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mparams\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     32\u001B[0m \u001B[43m            \u001B[49m\u001B[43mscoring\u001B[49m\n\u001B[1;32m     33\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [16]\u001B[0m, in \u001B[0;36mperform_grid_search\u001B[0;34m(X_train, y_train, pipeline, parameters, scoring)\u001B[0m\n\u001B[1;32m      9\u001B[0m pprint(parameters)\n\u001B[1;32m     11\u001B[0m t0 \u001B[38;5;241m=\u001B[39m time()\n\u001B[0;32m---> 13\u001B[0m \u001B[43mgs_clf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mDone in \u001B[39m\u001B[38;5;132;01m%0.3f\u001B[39;00m\u001B[38;5;124ms\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (time() \u001B[38;5;241m-\u001B[39m t0))\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28mprint\u001B[39m()\n",
      "File \u001B[0;32m~/miniconda3/envs/chatbot-sdia/lib/python3.9/site-packages/sklearn/model_selection/_search.py:875\u001B[0m, in \u001B[0;36mBaseSearchCV.fit\u001B[0;34m(self, X, y, groups, **fit_params)\u001B[0m\n\u001B[1;32m    869\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_results(\n\u001B[1;32m    870\u001B[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001B[1;32m    871\u001B[0m     )\n\u001B[1;32m    873\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[0;32m--> 875\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevaluate_candidates\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    877\u001B[0m \u001B[38;5;66;03m# multimetric is determined here because in the case of a callable\u001B[39;00m\n\u001B[1;32m    878\u001B[0m \u001B[38;5;66;03m# self.scoring the return type is only known after calling\u001B[39;00m\n\u001B[1;32m    879\u001B[0m first_test_score \u001B[38;5;241m=\u001B[39m all_out[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_scores\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/miniconda3/envs/chatbot-sdia/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1379\u001B[0m, in \u001B[0;36mGridSearchCV._run_search\u001B[0;34m(self, evaluate_candidates)\u001B[0m\n\u001B[1;32m   1377\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_run_search\u001B[39m(\u001B[38;5;28mself\u001B[39m, evaluate_candidates):\n\u001B[1;32m   1378\u001B[0m     \u001B[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001B[39;00m\n\u001B[0;32m-> 1379\u001B[0m     \u001B[43mevaluate_candidates\u001B[49m\u001B[43m(\u001B[49m\u001B[43mParameterGrid\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparam_grid\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/chatbot-sdia/lib/python3.9/site-packages/sklearn/model_selection/_search.py:852\u001B[0m, in \u001B[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001B[0;34m(candidate_params, cv, more_results)\u001B[0m\n\u001B[1;32m    845\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) \u001B[38;5;241m!=\u001B[39m n_candidates \u001B[38;5;241m*\u001B[39m n_splits:\n\u001B[1;32m    846\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    847\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcv.split and cv.get_n_splits returned \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    848\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minconsistent results. Expected \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    849\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msplits, got \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(n_splits, \u001B[38;5;28mlen\u001B[39m(out) \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m n_candidates)\n\u001B[1;32m    850\u001B[0m     )\n\u001B[0;32m--> 852\u001B[0m \u001B[43m_warn_or_raise_about_fit_failures\u001B[49m\u001B[43m(\u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43merror_score\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    854\u001B[0m \u001B[38;5;66;03m# For callable self.scoring, the return type is only know after\u001B[39;00m\n\u001B[1;32m    855\u001B[0m \u001B[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001B[39;00m\n\u001B[1;32m    856\u001B[0m \u001B[38;5;66;03m# can now be inserted with the correct key. The type checking\u001B[39;00m\n\u001B[1;32m    857\u001B[0m \u001B[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001B[39;00m\n\u001B[1;32m    858\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m callable(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscoring):\n",
      "File \u001B[0;32m~/miniconda3/envs/chatbot-sdia/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:367\u001B[0m, in \u001B[0;36m_warn_or_raise_about_fit_failures\u001B[0;34m(results, error_score)\u001B[0m\n\u001B[1;32m    360\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m num_failed_fits \u001B[38;5;241m==\u001B[39m num_fits:\n\u001B[1;32m    361\u001B[0m     all_fits_failed_message \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    362\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mAll the \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_fits\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m fits failed.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    363\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIt is very likely that your model is misconfigured.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    364\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou can try to debug the error by setting error_score=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mraise\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    365\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBelow are more details about the failures:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mfit_errors_summary\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    366\u001B[0m     )\n\u001B[0;32m--> 367\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(all_fits_failed_message)\n\u001B[1;32m    369\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    370\u001B[0m     some_fits_failed_message \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    371\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mnum_failed_fits\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m fits failed out of a total of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_fits\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    372\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe score on these train-test partitions for these parameters\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    376\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBelow are more details about the failures:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mfit_errors_summary\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    377\u001B[0m     )\n",
      "\u001B[0;31mValueError\u001B[0m: \nAll the 3 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n3 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/home/matthieu/miniconda3/envs/chatbot-sdia/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/home/matthieu/miniconda3/envs/chatbot-sdia/lib/python3.9/site-packages/sklearn/pipeline.py\", line 378, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"/home/matthieu/miniconda3/envs/chatbot-sdia/lib/python3.9/site-packages/sklearn/pipeline.py\", line 336, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"/home/matthieu/miniconda3/envs/chatbot-sdia/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n    return self.func(*args, **kwargs)\n  File \"/home/matthieu/miniconda3/envs/chatbot-sdia/lib/python3.9/site-packages/sklearn/pipeline.py\", line 870, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"/home/matthieu/miniconda3/envs/chatbot-sdia/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 2079, in fit_transform\n    X = super().fit_transform(raw_documents)\n  File \"/home/matthieu/miniconda3/envs/chatbot-sdia/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1338, in fit_transform\n    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n  File \"/home/matthieu/miniconda3/envs/chatbot-sdia/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1209, in _count_vocab\n    for feature in analyze(doc):\n  File \"/home/matthieu/miniconda3/envs/chatbot-sdia/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 113, in _analyze\n    doc = tokenizer(doc)\n  File \"/tmp/ipykernel_17605/2133088996.py\", line 4, in lemmatize_text\n    return process_text(text, \"lemmatize\", preprocessed)\n  File \"/tmp/ipykernel_17605/2133088996.py\", line 17, in process_text\n    processed_text = [token.lemma_ for token in text\nTypeError: 'int' object is not iterable\n"
     ]
    }
   ],
   "source": [
    "best_estimator_per_clf(X_train, y_train, gs_dict_tfidf, scoring=\"accuracy\")\n",
    "best_estimator_per_clf(X_train_embedded_avg, y_train, gs_dict_embeddings, scoring=\"accuracy\")\n",
    "\n",
    "# TODO : Use random state in gsCV and XGBoost for reporductibility\n",
    "\n",
    "# TODO : implement multiple scoring : https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py\n",
    "\n",
    "# TODO : extract metrics https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "pipeline: ['dectree']\n",
      "parameters:\n",
      "{'dectree__max_depth': [4, 10]}\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "\n",
      "Done in 0.412s\n",
      "\n",
      "Best score: 0.671\n",
      "Best parameters set:\n",
      "\t'dectree__max_depth': '4'\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "pipeline: ['svm_clf']\n",
      "parameters:\n",
      "{'svm_clf__kernel': ['linear', 'rbf']}\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "\n",
      "Done in 0.225s\n",
      "\n",
      "Best score: 0.829\n",
      "Best parameters set:\n",
      "\t'svm_clf__kernel': 'linear'\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "pipeline: ['log_reg']\n",
      "parameters:\n",
      "{}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "\n",
      "Done in 0.064s\n",
      "\n",
      "Best score: 0.816\n",
      "Best parameters set:\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "pipeline: ['random_forest']\n",
      "parameters:\n",
      "{}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "\n",
      "Done in 1.477s\n",
      "\n",
      "Best score: 0.777\n",
      "Best parameters set:\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "pipeline: ['skboost']\n",
      "parameters:\n",
      "{}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "\n",
      "Done in 10.705s\n",
      "\n",
      "Best score: 0.809\n",
      "Best parameters set:\n"
     ]
    }
   ],
   "source": [
    "best_estimator_per_clf(X_train_embedded_avg, y_train, gs_dict_embeddings, scoring=\"accuracy\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test = sum(process_text(nlp(\"I want to print 76 page of a document\"), mode=\"embed\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = gs_dict_embeddings['svm_clf']['gs'].best_estimator_\n",
    "model.predict([test])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Neural networks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model selection"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "idea : skew text classification if name entities are found (either by multiple channels NN or by adding a feature to the data passed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# LSTM : https://www.tensorflow.org/text/tutorials/text_classification_rnn\n",
    "len(nlp.vocab.vectors.keys())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "max_words = 30 # Max number of words in a sentence\n",
    "\n",
    "raw_inputs = X_train_embedded\n",
    "padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    X_train_embedded,\n",
    "    maxlen=max_words,\n",
    "    padding=\"pre\",\n",
    "    truncating=\"pre\",\n",
    "    dtype=\"float32\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "padded_inputs.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense, Input, Masking\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y_train)\n",
    "number_classes = len(y_train.unique())\n",
    "\n",
    "model=Sequential()\n",
    "#model.add(Embedding(vocab_size,300,input_length=max_words))\n",
    "model.add(Masking(mask_value=0, input_shape=(None, 300)))\n",
    "model.add(LSTM(units=128,\n",
    "               return_sequences=False,\n",
    "               input_shape=(None, 300)\n",
    "               ))\n",
    "model.add(Dense(number_classes, activation='softmax'))\n",
    "\n",
    "print(model.summary())\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(tf.convert_to_tensor(padded_inputs), y_encoded, epochs=20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test = process_text(\"tell me your name\", mode=\"embed\", preprocessed=False)\n",
    "predict = model.predict(np.asarray([test]))\n",
    "predicted_class = np.argmax(predict)\n",
    "predicted_class = le.inverse_transform([predicted_class])\n",
    "predicted_class"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Makes no sense to train LSTM / CNN on Tf-Idf : They preserve spatial / temporal information, but that information is lost with tfidf\n",
    "# Does not play to their strength, not more relevant than a classic classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Helper function\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "  plt.plot(history.history[metric])\n",
    "  plt.plot(history.history['val_'+metric], '')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(metric)\n",
    "  plt.legend([metric, 'val_'+metric])\n",
    "\n",
    "plot_graphs(history, \"accuracy\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def model_to_optimize(num_filters, kernel_size):\n",
    "    model = Sequential([\n",
    "    tf.keras.layers.Conv1D(num_filters, kernel_size, activation='relu'),\n",
    "    tf.keras.layers.GlobalMaxPooling1D(),\n",
    "    Dense(10, activation='relu'),\n",
    "    Dense(number_classes, activation='softmax')])\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "params = {\n",
    "    \"num_filters\":[32, 64, 128],\n",
    "    \"kernel_size\":[3, 5, 7],\n",
    "}\n",
    "\n",
    "model = tf.keras.wrappers.scikit_learn.KerasClassifier(build_fn=model_to_optimize,\n",
    "                            epochs=20,\n",
    "                           batch_size=10,\n",
    "                            verbose=False)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "search = GridSearchCV(estimator=model, param_grid=params,\n",
    "                              cv=2, verbose=1)\n",
    "search_result = search.fit(padded_inputs, y_encoded)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "search_result.best_params_\n",
    "pd.DataFrame(search.cv_results_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "import tensorflow as tf\n",
    "from keras.layers import TextVectorization\n",
    "from keras.layers import Embedding\n",
    "from keras import layers\n",
    "\n",
    "max_words = 30 # Max number of words in a sentence\n",
    "\n",
    "raw_inputs = X_train_embedded\n",
    "padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    X_train_embedded,\n",
    "    maxlen=max_words,\n",
    "    padding=\"pre\",\n",
    "    truncating=\"pre\",\n",
    "    dtype=\"float32\",\n",
    ")\n",
    "\n",
    "# Affichage des scores moyens par pli\n",
    "print('---------------------------------------------------------------------')\n",
    "print('Scores par pli')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "  print('---------------------------------------------------------------------')\n",
    "  print(f'> Pli {i+1} - Loss: {loss_per_fold[i]:.2f}',\n",
    "        f'- Accuracy: {acc_per_fold[i]:.2f}%')\n",
    "print('---------------------------------------------------------------------')\n",
    "print('Scores moyens pour tous les plis :')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold):.2f}',\n",
    "      f'(+- {np.std(acc_per_fold):.2f})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold):.2f}')\n",
    "print('---------------------------------------------------------------------')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "accuracy_data = []\n",
    "loss_data = []\n",
    "for i, h in enumerate(histories):\n",
    "  acc = h.history['acc']\n",
    "  val_acc = h.history['val_acc']\n",
    "  loss = h.history['loss']\n",
    "  val_loss = h.history['val_loss']\n",
    "  for j in range(len(acc)):\n",
    "    accuracy_data.append([i+1, j+1, acc[j], 'Entraînement'])\n",
    "    accuracy_data.append([i+1, j+1, val_acc[j], 'Validation'])\n",
    "    loss_data.append([i+1, j+1, loss[j], 'Entraînement'])\n",
    "    loss_data.append([i+1, j+1, val_loss[j], 'Validation'])\n",
    "\n",
    "acc_df = pd.DataFrame(accuracy_data,\n",
    "                      columns=['Pli', 'Epoch', 'Accuracy', 'Données'])\n",
    "sns.relplot(data=acc_df, x='Epoch', y='Accuracy', hue='Pli', style='Données',\n",
    "            kind='line')\n",
    "\n",
    "loss_df = pd.DataFrame(loss_data, columns=['Pli', 'Epoch', 'Perte', 'Données'])\n",
    "sns.relplot(data=loss_df, x='Epoch', y='Perte', hue='Pli', style='Données',\n",
    "            kind='line')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use party one to implement a CNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Tes with matrix embedding based solely on my vocab or on the whole spacy vocab"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TODO : Once data has been generated, apply vizualization techniques found in partie 1 to it !"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
