{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Our first approach is to generate, by hand, a simple collections of sentences organised by intents and associated answers.\n",
    "We will then process that data to make it suitable for NLP applications, encode it using \"bag of word\" and train a neural network to predict user intent from an utterance.\n",
    "\n",
    "Then, we wil used pre-trained word embeddings\n",
    "\n",
    "Then, we will generate training data using OpenAI GPT-3 API and train it, using both bag of words and word embeddings\n",
    "We can also try TF-IDF to compare it with the NN."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import / Generate data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn import tree, svm, naive_bayes\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_file = open('intents.json').read()\n",
    "intents = json.loads(data_file)\n",
    "\n",
    "\n",
    "data = []\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        data.append([pattern, intent['tag']])\n",
    "\n",
    "df = pd.DataFrame(data, columns=['text','intent'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data pre-processing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# fine-tune preprocessing for spaCy word embeddings using this method : https://www.kaggle.com/code/christofhenkel/how-to-preprocessing-when-using-embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def lemmatize_text(text, preprocessed=True):\n",
    "    return process_text(text, \"lemmatize\", preprocessed)\n",
    "\n",
    "def tokenize_text(text, preprocessed=True):\n",
    "    return process_text(text, \"tokenize\", preprocessed)\n",
    "\n",
    "def process_text(text, mode: str, preprocessed=True):\n",
    "    if not preprocessed:\n",
    "        text = nlp(text)\n",
    "    if mode == \"tokenize\":\n",
    "        processed_text = [token for token in text] # token and embed must have the same processing + SpaCy provides embeddings for punctuation\n",
    "    elif mode == \"embed\":\n",
    "        processed_text = [token.vector for token in text] # token and embed must have the same processing\n",
    "    elif mode == \"lemmatize\":\n",
    "        processed_text = [token.lemma_ for token in text\n",
    "                               if not token.is_punct and not token.is_space and not token.like_url and not token.like_email]\n",
    "    else:\n",
    "        raise ValueError(\"Mode not supported\")\n",
    "    return processed_text\n",
    "\n",
    "def save_preprocessed(raw_text, save_path):\n",
    "    doc_bin = DocBin(attrs=[\"LEMMA\", \"ENT_IOB\", \"ENT_TYPE\"], store_user_data=True)\n",
    "    for doc in nlp.pipe(raw_text):\n",
    "        doc_bin.add(doc)\n",
    "    # save DocBin to a file on disc\n",
    "    doc_bin.to_disk(save_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_name_spacy = 'preprocessed_documents.spacy'\n",
    "#save_preprocessed(raw_text=df[\"text\"], save_path=\"preprocessed_documents.spacy\")\n",
    "\n",
    "# Load DocBin at later time or on different system from disc or bytes object\n",
    "doc_bin = DocBin().from_disk(file_name_spacy)\n",
    "df[\"doc\"] = list(doc_bin.get_docs(nlp.vocab))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train = df[\"doc\"]\n",
    "y_train = df[\"intent\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 1), lowercase=False, tokenizer=tokenize_text, max_features=3000)\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_train_embedded = df[\"doc\"].apply(process_text, args=(\"embed\", True,))\n",
    "X_train_embedded_avg = X_train_embedded.apply(sum).apply(pd.Series)\n",
    "\n",
    "# The following code block construct a sentence representation as a sum of all embeddings of the words in it, weighted by their tfidf score\n",
    "\n",
    "result = []\n",
    "for (idxRow, sentence) in X_train.items():\n",
    "    final = 0\n",
    "    for idxWord, word in enumerate(sentence):\n",
    "        tfidf_idx = vectorizer.vocabulary_[word]\n",
    "        final += (X_train_tfidf.toarray())[idxRow][tfidf_idx] * X_train_embedded[idxRow][idxWord]\n",
    "    result.append(final)\n",
    "\n",
    "X_train_embedded_avg_tfidf = pd.Series(result).apply(pd.Series)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Models preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(lowercase=False, tokenizer=lemmatize_text, max_features=3000)\n",
    "\n",
    "# classifiers to use\n",
    "\n",
    "# TODO : implement more classifiers\n",
    "\n",
    "gs_dict_tfidf = defaultdict(dict)\n",
    "\n",
    "dectree = tree.DecisionTreeClassifier()\n",
    "svm_clf = svm.SVC()\n",
    "multi_nb = naive_bayes.MultinomialNB() # Not suitable for negative values (thus not suitable for word embeddings)\n",
    "\n",
    "gs_dict_tfidf['dectree']['pipeline'] = Pipeline([\n",
    "    ('vect', vect),\n",
    "    ('dectree', dectree)])\n",
    "gs_dict_tfidf['svm_clf']['pipeline'] = Pipeline([\n",
    "    ('vect', vect),\n",
    "    ('svm_clf', svm_clf)])\n",
    "gs_dict_tfidf['multi_nb']['pipeline'] = Pipeline([\n",
    "    ('vect', vect),\n",
    "    ('multi_nb', multi_nb)])\n",
    "\n",
    "gs_dict_tfidf['dectree']['params'] = {\n",
    "    \"dectree__max_depth\": [4, 40],\n",
    "    \"vect__ngram_range\": ((1, 1), (1, 2), (1,3), (1,4)),\n",
    "    \"vect__use_idf\": (True, False),\n",
    "    \"vect__binary\": (True, False),\n",
    "}\n",
    "gs_dict_tfidf['svm_clf']['params'] = {\n",
    "    \"svm_clf__kernel\": [\"linear\", \"rbf\"],\n",
    "    \"vect__ngram_range\": ((1, 1), (1, 2), (1,3), (1,4)),\n",
    "    \"vect__use_idf\": (True, False),\n",
    "    \"vect__binary\": (True, False),\n",
    "}\n",
    "gs_dict_tfidf['multi_nb']['params'] = {\n",
    "    \"multi_nb__alpha\": [0.00001, 0.0001, 0.001, 0.1, 1, 10, 100,1000],\n",
    "    \"vect__ngram_range\": ((1, 1), (1, 2), (1,3), (1,4)),\n",
    "    \"vect__use_idf\": (True, False),\n",
    "    \"vect__binary\": (True, False),\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "^Classic below : WORD EMBEDDINGS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "gs_dict_embeddings = defaultdict(dict)\n",
    "# classifiers to use\n",
    "dectree = tree.DecisionTreeClassifier()\n",
    "svm_clf = svm.SVC()\n",
    "\n",
    "gs_dict_embeddings['dectree']['pipeline'] = Pipeline([\n",
    "    ('dectree', dectree)])\n",
    "gs_dict_embeddings['svm_clf']['pipeline'] = Pipeline([\n",
    "    ('svm_clf', svm_clf)])\n",
    "\n",
    "gs_dict_embeddings['dectree']['params'] = {\n",
    "    \"dectree__max_depth\": [4, 10],\n",
    "}\n",
    "gs_dict_embeddings['svm_clf']['params'] = {\n",
    "    \"svm_clf__kernel\": [\"linear\", \"rbf\"],\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Selection"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def perform_grid_search(X_train, y_train, pipeline, parameters, scoring):\n",
    "    gs_clf = GridSearchCV(pipeline, parameters, n_jobs=1, verbose=1, cv=3, scoring=scoring) # Issue when n_jobs = -1 OR > 1\n",
    "    # I believe this may be because we use a custom tokenizer in TfidfVectorizer(), can't find how to solve it\n",
    "    print(\"\\n------------------------------------------------------------------------\\n\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "\n",
    "    t0 = time()\n",
    "\n",
    "    gs_clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"\\nDone in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % gs_clf.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(f\"\\t'{param_name}': '{best_parameters[param_name]}'\")\n",
    "    return gs_clf\n",
    "\n",
    "def best_estimator_per_clf(X_train, y_train, gs_dict: defaultdict, scoring):\n",
    "    for clf in dict(gs_dict):\n",
    "        gs_dict[clf]['gs'] = perform_grid_search(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            gs_dict[clf]['pipeline'],\n",
    "            gs_dict[clf]['params'],\n",
    "            scoring\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Traditional classifiers cannot accept as input a sequence of word embeddings, so we try 2 strategies :\n",
    "- Averaging all embeddings in a sentence\n",
    "- Averaging using TF-IDF score as weight"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "pipeline: ['dectree']\n",
      "parameters:\n",
      "{'dectree__max_depth': [4, 10]}\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "\n",
      "Done in 0.066s\n",
      "\n",
      "Best score: 0.421\n",
      "Best parameters set:\n",
      "\t'dectree__max_depth': '10'\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "pipeline: ['svm_clf']\n",
      "parameters:\n",
      "{'svm_clf__kernel': ['linear', 'rbf']}\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "\n",
      "Done in 0.042s\n",
      "\n",
      "Best score: 0.739\n",
      "Best parameters set:\n",
      "\t'svm_clf__kernel': 'linear'\n"
     ]
    }
   ],
   "source": [
    "best_estimator_per_clf(X_train_embedded_avg_tfidf, y_train, gs_dict_embeddings, scoring=\"accuracy\")\n",
    "\n",
    "# TODO : implement multiple scoring : https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py\n",
    "\n",
    "# TODO : extract metrics https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 4.61637974e-01,  1.13560700e+00, -1.02110004e+00,  8.33649635e-02,\n        5.00504971e-01,  2.61418998e-01, -3.87266040e-01, -1.54758000e+00,\n        5.03413826e-02,  1.57266893e+01, -2.69594002e+00,  9.97252107e-01,\n        1.69486105e+00, -4.74574983e-01, -1.32837963e+00, -4.22390997e-01,\n       -8.19185078e-01,  1.50653019e+01, -1.59457624e+00, -1.56144702e+00,\n        6.65617049e-01, -8.29769969e-02, -2.71458983e-01,  2.03112975e-01,\n        6.40709996e-01,  1.32894230e+00, -1.56676382e-01, -2.17072773e+00,\n       -3.94010007e-01,  4.64303017e-01,  5.08952022e-01,  1.70764005e+00,\n        8.82860124e-02, -5.47645867e-01, -8.17725062e-01, -2.17576098e+00,\n       -1.03687799e+00,  3.65506977e-01,  2.05558538e-03, -7.48545110e-01,\n        1.99170977e-01,  1.29109299e+00,  1.56570300e-02, -2.27649307e+00,\n       -9.02953029e-01,  2.60586470e-01, -3.59949052e-01,  1.38292694e+00,\n        8.08113277e-01,  8.23220015e-02, -2.31747699e+00,  1.25209391e-02,\n        9.51222003e-01, -9.58997130e-01,  1.11894000e+00, -1.99999988e-01,\n        2.18890041e-01, -7.62199044e-01,  5.15177011e-01, -9.81196642e-01,\n       -9.58328426e-01,  3.84967089e-01, -1.76092887e+00,  2.41898990e+00,\n        1.48657799e+00, -4.08674061e-01, -1.77194810e+00,  1.53124094e+00,\n        1.27030778e+00,  2.14280081e+00,  1.09716403e+00,  7.17959344e-01,\n        1.79668796e+00, -1.90153956e-01,  3.34786892e+00, -4.47250158e-02,\n        8.30543995e-01, -9.37943995e-01,  2.83339947e-01,  1.20369887e+00,\n        1.68664753e-01,  9.98257875e-01, -7.99109936e-01,  4.18013334e-02,\n        4.51859981e-01, -1.74207032e+00,  3.23270947e-01, -1.20614791e+00,\n        4.01826286e+00,  1.11890697e+00, -1.88996005e+00, -5.33331037e-01,\n        1.80975795e-01, -1.45143062e-01,  1.40679288e+00,  1.83837384e-01,\n        1.22500192e-02, -8.15329924e-02, -3.68207157e-01, -1.35420609e+00,\n       -4.87031996e-01, -8.14728022e-01, -2.05056041e-01, -2.38661990e-01,\n       -3.86915982e-01, -3.30228996e+00,  2.52537012e+00,  1.53799200e+00,\n       -2.62607038e-01, -3.44729036e-01,  2.52554983e-01, -1.41091990e+00,\n        9.51863945e-01, -3.96870300e-02,  3.08457017e-01, -2.25598902e-01,\n       -6.31348968e-01,  1.14241958e-01, -2.70224571e-01, -2.18315005e-01,\n        1.57061195e+00, -4.64805573e-01, -3.68147016e-01,  8.79652023e-01,\n        4.01201010e-01,  1.80539203e+00, -6.13775015e-01, -1.51771998e+00,\n        5.90641558e-01, -3.22565943e-01, -4.97981966e-01,  8.55550885e-01,\n       -1.55404079e+00,  1.32010114e+00,  5.58181047e-01, -9.40377951e-01,\n       -1.96345007e+00,  1.06191075e+00, -1.49370599e+00, -8.04297253e-03,\n       -1.00896387e+01, -2.22429782e-01, -4.22656983e-01,  6.27389967e-01,\n        8.94972980e-01, -2.10833192e+00, -3.87107015e-01,  6.58048987e-01,\n        1.07174003e+00, -1.60385299e+00,  8.67515981e-01, -8.49170238e-02,\n       -5.68284988e-01, -1.88062996e-01, -6.20144963e-01,  1.18468201e+00,\n       -2.44394374e+00,  6.58170998e-01, -9.87569243e-03,  5.79912007e-01,\n       -3.56758982e-01,  2.15430036e-02, -6.41444981e-01,  2.53626019e-01,\n       -2.44915485e+00, -1.31381297e+00, -4.50990021e-01, -7.92954206e-01,\n        1.41615307e+00, -3.36610004e-02, -2.00506002e-01, -4.06991124e-01,\n        1.77068090e+00, -2.91404098e-01,  1.40014303e+00,  1.61149895e+00,\n        3.76473665e-01,  7.93320537e-02, -1.12799299e+00,  5.75775027e-01,\n       -2.32969731e-01, -3.81127000e-01, -6.09338045e-01, -1.89094067e+00,\n        1.77504033e-01,  6.41710341e-01, -1.09495199e+00, -1.25180036e-01,\n       -4.95289981e-01, -8.13054979e-01, -4.20969039e-01,  1.05171502e+00,\n       -1.88063025e-01,  7.54984975e-01,  2.09928203e+00,  1.04910707e+00,\n       -7.92103052e-01, -1.90520024e+00,  3.98717999e-01,  6.64169863e-02,\n        1.94006407e+00, -1.49602997e+00, -1.94403100e+00, -1.05585504e+00,\n        2.57961297e+00,  1.35367811e+00, -3.09387028e-01, -9.87366885e-02,\n       -1.96805501e+00,  1.46554029e+00,  1.26691496e+00, -7.79242992e-01,\n       -1.62788606e+00, -7.47346342e-01, -4.34875995e-01,  2.52438402e+00,\n       -1.50551307e+00, -3.02019030e-01, -2.81024408e+00, -1.45778030e-01,\n        1.92193985e-01, -1.32910371e+00,  8.39542031e-01,  2.64848024e-01,\n       -3.27234000e-01,  7.82022953e-01,  1.34421098e+00,  1.80137706e+00,\n        9.59219992e-01, -1.17480993e+00, -2.17819381e+00, -3.58649015e-01,\n        8.58550668e-01,  3.46962363e-01, -5.99054098e-01, -9.28461075e-01,\n       -1.10078192e+00,  3.78470957e-01, -7.24777579e-01,  1.43141794e+00,\n        1.59488893e+00,  6.57455742e-01,  9.46488023e-01,  6.97373092e-01,\n        1.27876997e+00, -1.99881220e+00,  2.05380011e+00, -5.01549959e-01,\n       -1.20948911e+00, -2.88939953e-01,  7.75819570e-02, -1.19797945e-01,\n       -4.03746963e-01,  1.59945297e+00,  1.08760297e+00,  3.00869656e+00,\n        1.77213013e-01, -9.05419052e-01, -4.43906099e-01, -1.17956042e+00,\n        9.77252841e-01,  1.29532707e+00,  9.55682039e-01,  1.12232089e+00,\n        1.72128201e+00, -1.90753996e+00,  9.41110015e-01,  5.10607541e-01,\n        5.85908031e+00,  1.63441032e-01, -1.32316387e+00, -1.29675996e+00,\n       -2.52921283e-01, -3.57689917e-01,  1.36189306e+00, -8.37140158e-02,\n       -1.83973408e+00,  1.70740008e-01, -7.82493651e-01, -7.92130828e-02,\n        1.66523492e+00,  7.40939021e-01, -1.10024214e+00, -7.46369004e-01,\n       -7.65621006e-01, -1.52610207e+00,  3.70159030e-01,  9.25000310e-02,\n        2.21218988e-01, -1.60658503e+00, -1.00590193e+00,  4.47962403e-01,\n        9.35340047e-01, -2.07307264e-02,  1.51449502e+00, -1.40035701e+00,\n       -1.97107399e+00,  2.54380256e-02,  1.62895489e+00,  1.04460084e+00],\n      dtype=float32)"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = sum(process_text(nlp(\"I want to print 76 page of a document\"), mode=\"embed\"))\n",
    "test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "skew text classification if name entities are found (either by multiple channels NN or by adding a feature to the data passed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "# LSTM : https://www.tensorflow.org/text/tutorials/text_classification_rnn"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
