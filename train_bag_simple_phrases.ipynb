{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Our first approach is to generate, by hand, a simple collections of sentences organised by intents and associated answers.\n",
    "We will then process that data to make it suitable for NLP applications, encode it using \"bag of word\" and train a neural network to predict user intent from an utterance.\n",
    "\n",
    "Then, we wil used pre-trained word embeddings\n",
    "\n",
    "Then, we will generate training data using OpenAI GPT-3 API and train it, using both bag of words and word embeddings\n",
    "We can also try TF-IDF to compare it with the NN."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import / Generate data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "data_file = open('intents.json').read()\n",
    "intents = json.loads(data_file)\n",
    "\n",
    "\n",
    "data = []\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        data.append([pattern, intent['tag']])\n",
    "\n",
    "df = pd.DataFrame(data, columns=['text','intent'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "                                      text            intent\n13                           Who are you ?             about\n29             I want to raise a complaint         complaint\n12            Perfect, thank you very much            thanks\n2                        Is anyone there ?          greeting\n24      I want to print 46 pages of my_doc  printing_request\n25  Can you help me get 64 pages of doc4 ?  printing_request\n22                What can you do for me ?              help\n14                          What are you ?             about\n27                Print 78 pages from doc8  printing_request\n8                                   Thanks            thanks",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>intent</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>13</th>\n      <td>Who are you ?</td>\n      <td>about</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>I want to raise a complaint</td>\n      <td>complaint</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Perfect, thank you very much</td>\n      <td>thanks</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Is anyone there ?</td>\n      <td>greeting</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>I want to print 46 pages of my_doc</td>\n      <td>printing_request</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>Can you help me get 64 pages of doc4 ?</td>\n      <td>printing_request</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>What can you do for me ?</td>\n      <td>help</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>What are you ?</td>\n      <td>about</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>Print 78 pages from doc8</td>\n      <td>printing_request</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Thanks</td>\n      <td>thanks</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data pre-processing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# Helper function\n",
    "\n",
    "def lemmatize_text(text, preprocessed=False):\n",
    "    if not preprocessed:\n",
    "        text = nlp(text)\n",
    "    lemmatized_texts = [token.lemma_ for token in text\n",
    "                               if not token.is_punct and not token.is_space and not token.like_url and not token.like_email]\n",
    "    return lemmatized_texts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "doc_bin = DocBin(attrs=[\"LEMMA\", \"ENT_IOB\", \"ENT_TYPE\"], store_user_data=True)\n",
    "\n",
    "for doc in nlp.pipe(df['text']):\n",
    "    doc_bin.add(doc)\n",
    "\n",
    "# save DocBin to a file on disc\n",
    "file_name_spacy = 'preprocessed_documents.spacy'\n",
    "doc_bin.to_disk(file_name_spacy)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "#Load DocBin at later time or on different system from disc or bytes object\n",
    "doc_bin = DocBin().from_disk(file_name_spacy)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "[Hi, Hey, Is anyone there ?, Hello, Good morning !, Bye, See you later, Goodbye, Thanks, Thank you, That's great, Thanks for the help, Perfect, thank you very much, Who are you ?, What are you ?, What is this, What is your name ?, What should I call you ?, What is your name ?, Could you help me ?, Give me a hand please, Can you help ?, What can you do for me ?, I need help, I want to print 46 pages of my_doc, Can you help me get 64 pages of doc4 ?, Get me 6 pages of my_file, Print 78 pages from doc8, I have a complaint, I want to raise a complaint, I am not satisfied]\n"
     ]
    }
   ],
   "source": [
    "docs = list(doc_bin.get_docs(nlp.vocab))\n",
    "print(len(docs))\n",
    "print(docs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "0             greeting\n1             greeting\n2             greeting\n3             greeting\n4             greeting\n5              goodbye\n6              goodbye\n7              goodbye\n8               thanks\n9               thanks\n10              thanks\n11              thanks\n12              thanks\n13               about\n14               about\n15               about\n16                name\n17                name\n18                name\n19                help\n20                help\n21                help\n22                help\n23                help\n24    printing_request\n25    printing_request\n26    printing_request\n27    printing_request\n28           complaint\n29           complaint\n30           complaint\nName: intent, dtype: object"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"doc\"] = docs\n",
    "X_train = df[\"doc\"].apply(lemmatize_text, args=(True,))\n",
    "y_train = df[\"intent\"]\n",
    "y_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "0                                             [hi]\n1                                            [hey]\n2                              [be, anyone, there]\n3                                          [hello]\n4                                  [good, morning]\n5                                            [bye]\n6                                [see, you, later]\n7                                        [goodbye]\n8                                          [thank]\n9                                     [thank, you]\n10                               [that, be, great]\n11                         [thank, for, the, help]\n12               [perfect, thank, you, very, much]\n13                                  [who, be, you]\n14                                 [what, be, you]\n15                                [what, be, this]\n16                          [what, be, your, name]\n17                    [what, should, I, call, you]\n18                          [what, be, your, name]\n19                           [could, you, help, I]\n20                      [give, I, a, hand, please]\n21                                [can, you, help]\n22                    [what, can, you, do, for, I]\n23                                 [I, need, help]\n24      [I, want, to, print, 46, page, of, my_doc]\n25    [can, you, help, I, get, 64, page, of, doc4]\n26                  [get, I, 6, page, of, my_file]\n27                   [print, 78, page, from, doc8]\n28                         [I, have, a, complaint]\n29              [I, want, to, raise, a, complaint]\n30                         [I, be, not, satisfied]\nName: doc, dtype: object"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'dectree']\n",
      "parameters:\n",
      "{'dectree__max_depth': [4, 10],\n",
      " 'vect__ngram_range': ((1, 1), (1, 2), (1, 3)),\n",
      " 'vect__use_idf': (True, False)}\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthieu/miniconda3/envs/chatbot-sdia/lib/python3.9/site-packages/sklearn/model_selection/_split.py:684: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.289s\n",
      "\n",
      "Best score: 0.548\n",
      "Best parameters set:\n",
      "\tdectree__max_depth: 10\n",
      "\tvect__ngram_range: (1, 2)\n",
      "\tvect__use_idf: True\n"
     ]
    }
   ],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(1, 2), lowercase=False, tokenizer=lambda x: x, max_features=3000)\n",
    "\n",
    "# classifier to use\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "\n",
    "intent_clf = Pipeline([\n",
    "    ('vect', vect),\n",
    "    ('dectree', clf)])\n",
    "\n",
    "parameters = {\n",
    "    \"dectree__max_depth\": [4, 10],\n",
    "    \"vect__ngram_range\": ((1, 1), (1, 2), (1,3)),\n",
    "    \"vect__use_idf\": (True, False),\n",
    "}\n",
    "\n",
    "gs_clf = GridSearchCV(intent_clf, parameters, n_jobs=-1, verbose=1, cv=5)\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in intent_clf.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters)\n",
    "t0 = time()\n",
    "\n",
    "gs_clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % gs_clf.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = gs_clf.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "['I', 'want', 'to', 'print', 'a', 'document']"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lematize_text(\"I want to print a document\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "classes = df[\"intent\"].unique()\n",
    "words = set() # change words to vocab"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "pickle.dump(words, open('words.pkl', 'wb'))\n",
    "pickle.dump(classes, open('classes.pkl', 'wb'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "documents = pickle.load(open('words.pkl', 'rb'))\n",
    "classes = pickle.load(open('classes.pkl', 'rb'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
