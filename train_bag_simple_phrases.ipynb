{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Our first approach is to generate, by hand, a simple collections of sentences organised by intents and associated answers.\n",
    "We will then process that data to make it suitable for NLP applications, encode it using \"bag of word\" and train a neural network to predict user intent from an utterance.\n",
    "\n",
    "Then, we wil used pre-trained word embeddings\n",
    "\n",
    "Then, we will generate training data using OpenAI GPT-3 API and train it, using both bag of words and word embeddings\n",
    "We can also try TF-IDF to compare it with the NN."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import / Generate data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "data_file = open('intents.json').read()\n",
    "intents = json.loads(data_file)\n",
    "\n",
    "\n",
    "data = []\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        data.append([pattern, intent['tag']])\n",
    "\n",
    "df = pd.DataFrame(data, columns=['text','intent'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data pre-processing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "# Helper function\n",
    "\n",
    "def lematize_text(text, preprocessed=False):\n",
    "    if not preprocessed:\n",
    "        text = nlp(text)\n",
    "    lematized_texts = [token.lemma_ for token in text\n",
    "                               if not token.is_punct and not token.is_space and not token.like_url and not token.like_email]\n",
    "    return lematized_texts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "doc_bin = DocBin(attrs=[\"LEMMA\", \"ENT_IOB\", \"ENT_TYPE\"], store_user_data=True)\n",
    "\n",
    "for doc in nlp.pipe(df['text'].str.lower()):\n",
    "    doc_bin.add(doc)\n",
    "\n",
    "# save DocBin to a file on disc\n",
    "file_name_spacy = 'preprocessed_documents.spacy'\n",
    "doc_bin.to_disk(file_name_spacy)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "#Load DocBin at later time or on different system from disc or bytes object\n",
    "doc_bin = DocBin().from_disk(file_name_spacy)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "<spacy.tokens._serialize.DocBin at 0x7ff668c148e0>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_bin"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "[hi, hey, is anyone there ?, hello, good morning !, bye, see you later, goodbye, thanks, thank you, that's great, thanks for the help, perfect, thank you very much, who are you ?, what are you ?, what is this, what is your name ?, what should i call you ?, what is your name ?, could you help me ?, give me a hand please, can you help ?, what can you do for me ?, i need help, i want to print 46 pages of my_doc, can you help me get 64 pages of doc4 ?, get me 6 pages of my_file, print 78 pages from doc8, i have a complaint, i want to raise a complaint, i am not satisfied]\n"
     ]
    }
   ],
   "source": [
    "docs = list(doc_bin.get_docs(nlp.vocab))\n",
    "print(len(docs))\n",
    "print(docs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "df[\"doc\"] = docs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "X_train = df[\"doc\"].apply(lematize_text, args=(True,))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "data": {
      "text/plain": "0                                             [hi]\n1                                            [hey]\n2                              [be, anyone, there]\n3                                          [hello]\n4                                  [good, morning]\n5                                            [bye]\n6                                [see, you, later]\n7                                        [goodbye]\n8                                          [thank]\n9                                     [thank, you]\n10                               [that, be, great]\n11                         [thank, for, the, help]\n12               [perfect, thank, you, very, much]\n13                                  [who, be, you]\n14                                 [what, be, you]\n15                                [what, be, this]\n16                          [what, be, your, name]\n17                    [what, should, I, call, you]\n18                          [what, be, your, name]\n19                           [could, you, help, I]\n20                      [give, I, a, hand, please]\n21                                [can, you, help]\n22                    [what, can, you, do, for, I]\n23                                 [I, need, help]\n24      [I, want, to, print, 46, page, of, my_doc]\n25    [can, you, help, I, get, 64, page, of, doc4]\n26                  [get, I, 6, page, of, my_file]\n27                   [print, 78, page, from, doc8]\n28                         [I, have, a, complaint]\n29              [I, want, to, raise, a, complaint]\n30                         [I, be, not, satisfied]\nName: doc, dtype: object"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "df[\"doc\"] = [lematize_text(doc, preprocessed=True) for doc in docs]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "DecisionTreeClassifier(max_depth=4)\n"
     ]
    }
   ],
   "source": [
    "# classifier to use\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), lowercase=False, tokenizer=lambda x: x, max_features=3000)\n",
    "\n",
    "pipeline = Pipeline([('vect', vectorizer), ('dectree', clf)])\n",
    "parameters = {'dectree__max_depth':[4, 10]}\n",
    "\n",
    "gs_clf = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, cv=5)\n",
    "gs_clf.fit(X_train, y_train)\n",
    "\n",
    "print(gs_clf.best_estimator_.get_params()['dectree'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "['I', 'want', 'to', 'print', 'a', 'document']"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lematize_text(\"I want to print a document\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0, 0, 0, 0, 0, 0])"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(\"I want to print a document\")\n",
    "lemnas =  [token.lemma_ for token in nlp(\"I want to print a document\")\n",
    "                               if not token.is_punct and not token.is_space and not token.like_url and not token.like_email]\n",
    "gs_clf.predict(lemnas)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "classes = df[\"intent\"].unique()\n",
    "words = set() # change words to vocab"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "pickle.dump(words, open('words.pkl', 'wb'))\n",
    "pickle.dump(classes, open('classes.pkl', 'wb'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "documents = pickle.load(open('words.pkl', 'rb'))\n",
    "classes = pickle.load(open('classes.pkl', 'rb'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['greeting', 'goodbye', 'thanks', 'about', 'name', 'help',\n       'printing_request', 'complaint'], dtype=object)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# Training datasets\n",
    "# =============================================================\n",
    "# create our training data\n",
    "training = []\n",
    "# create an empty array for our output\n",
    "output_empty = [0] * len(classes)\n",
    "# training set, bag of words for each sentence\n",
    "for doc in documents:\n",
    "    # initialize our bag of words\n",
    "    bag = []\n",
    "    # list of tokenized words for the pattern\n",
    "    pattern_words = doc[0]\n",
    "    # lemmatize each word - create base word, in attempt to represent related words\n",
    "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
    "    # create our bag of words array with 1, if word match found in current pattern\n",
    "    for word in words:\n",
    "        bag.append(1) if word in pattern_words else bag.append(0)\n",
    "\n",
    "    # output is a '0' for each tag and '1' for current tag (for each pattern)\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "\n",
    "    training.append([bag, output_row])\n",
    "# shuffle our features and turn into np.array\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "# create train and test lists. X - patterns, Y - intents\n",
    "train_x = list(training[:, 0])\n",
    "train_y = list(training[:, 1])\n",
    "print(\"Training data created\")\n",
    "print(train_y)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
